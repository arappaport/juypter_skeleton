{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T16:40:03.302992Z",
     "start_time": "2024-03-23T16:40:03.299997Z"
    }
   },
   "outputs": [],
   "source": [
    "#Basic notebook with a few jupyter examples\n",
    "\n",
    "\n",
    "# Metrics to calc\n",
    "\n",
    "# Users that haven't purchased in a while\n",
    "# Last purchase per user - any product\n",
    "\n",
    "#Last purchase per user - each product\n",
    "#  * somehow include no purchases - for ones that they have been assigned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:40:03.758292Z",
     "start_time": "2024-03-23T16:40:03.756463Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log level = 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "LOGGER.debug(\"log level = %s\", str(LOGGER.getEffectiveLevel()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:40:04.161471Z",
     "start_time": "2024-03-23T16:40:04.157733Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DATA_PATH=\"../data/purchases.csv\"\n",
    "DATE_AS_OF=date.fromisoformat('2023-06-01') #Date for analysis\n",
    "#For data visualization - date range sizes\n",
    "DATE_GROUP_DAYS=30\n",
    "CUTOFF_DAYS=90\n",
    "\n",
    "\n",
    "\n",
    "REQUIRED_COLUMNS=['user', 'date','product', 'price']\n",
    "\n",
    "#loist of recommended actions for different date thresholds\n",
    "ACTION_DATES = dict(out_of_warranty=30, obsolete=90)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:40:04.862405Z",
     "start_time": "2024-03-23T16:40:04.856229Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:40:05.412271Z",
     "start_time": "2024-03-23T16:40:05.410398Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "def clean_raw(df:pd.DataFrame, required_columns:[str])->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Check and clean purchases dataframe.  \n",
    "    :param df: \n",
    "    :return:    updated dataframe  or exception if issue. \n",
    "    \"\"\"\n",
    "    \n",
    "    missing_cols = set(required_columns) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise Exception(f\"Invalid dataframe Missing required columns: {', '.join(missing_cols)}\")\n",
    "    \n",
    "    #trim up - sometimes there are bad rows at the bottom of the csv. We use the first required column\n",
    "    df = df.loc[df[required_columns[0]].notnull()]\n",
    "    \n",
    "    #Error if any of the cells are blanks\n",
    "    if df.isnull().sum().sum():\n",
    "        raise Exception(f\"Invalid dataframe: some cells are empty\")\n",
    "\n",
    "    #convert dates\n",
    "    df['date']  = pd.to_datetime(df['date'], format='%Y-%m-%d').dt.date\n",
    "     \n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:40:05.877971Z",
     "start_time": "2024-03-23T16:40:05.874720Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded 22 rows from CSV[../data/purchases.csv]\n"
     ]
    },
    {
     "data": {
      "text/plain": "  user        date   product  price\n0    A  2023-05-31    prod-a   17.0\n1    B  2023-06-01    prod-a    NaN\n2    C  2023-05-01    prod-b  256.0\n3    D  2023-06-01    prod-a  118.0\n4    D  2023-05-15    prod-a  224.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>date</th>\n      <th>product</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n      <td>2023-05-31</td>\n      <td>prod-a</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B</td>\n      <td>2023-06-01</td>\n      <td>prod-a</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C</td>\n      <td>2023-05-01</td>\n      <td>prod-b</td>\n      <td>256.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D</td>\n      <td>2023-06-01</td>\n      <td>prod-a</td>\n      <td>118.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D</td>\n      <td>2023-05-15</td>\n      <td>prod-a</td>\n      <td>224.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "LOGGER.info(\"Loaded %d rows from CSV[%s]\", len(df), DATA_PATH)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:40:06.578579Z",
     "start_time": "2024-03-23T16:40:06.566977Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  user        date   product  price\n0    A  2023-05-31    PROD-A   17.0\n1    B  2023-06-01    PROD-A    NaN\n2    C  2023-05-01    PROD-B  256.0\n3    D  2023-06-01    PROD-A  118.0\n4    D  2023-05-15    PROD-A  224.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>date</th>\n      <th>product</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n      <td>2023-05-31</td>\n      <td>PROD-A</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B</td>\n      <td>2023-06-01</td>\n      <td>PROD-A</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C</td>\n      <td>2023-05-01</td>\n      <td>PROD-B</td>\n      <td>256.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D</td>\n      <td>2023-06-01</td>\n      <td>PROD-A</td>\n      <td>118.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D</td>\n      <td>2023-05-15</td>\n      <td>PROD-A</td>\n      <td>224.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXAMPLE: of separate Lambda - this is overkill for this small task \n",
    "def to_caps(s:str) ->str: \n",
    "    return s.upper()\n",
    "\n",
    "df['product'] = df['product'].apply(to_caps)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:40:08.877458Z",
     "start_time": "2024-03-23T16:40:08.871334Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  user        date   product  price\n0    A  2023-05-31    prod-a   17.0\n1    B  2023-06-01    prod-a    NaN\n2    C  2023-05-01    prod-b  256.0\n3    D  2023-06-01    prod-a  118.0\n4    D  2023-05-15    prod-a  224.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>date</th>\n      <th>product</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n      <td>2023-05-31</td>\n      <td>prod-a</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B</td>\n      <td>2023-06-01</td>\n      <td>prod-a</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C</td>\n      <td>2023-05-01</td>\n      <td>prod-b</td>\n      <td>256.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D</td>\n      <td>2023-06-01</td>\n      <td>prod-a</td>\n      <td>118.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D</td>\n      <td>2023-05-15</td>\n      <td>prod-a</td>\n      <td>224.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXAMPLE: Lambda of in-line lambda\n",
    "df['product'] = df['product'].apply(lambda x:str(x).lower())\n",
    "df.head()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:40:10.282403Z",
     "start_time": "2024-03-23T16:40:10.274609Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXAMPLE: Convert a string date to a date type   "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#EXAMPLE:   Convert a column to dates - this can also be done in df csv load. \n",
    "df['date']  = pd.to_datetime(df['date'], format='%Y-%m-%d').dt.date\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:40:14.803560Z",
     "start_time": "2024-03-23T16:40:14.796031Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  user        date   product  price  days_ago\n0    A  2023-05-31    prod-a   17.0         1\n1    B  2023-06-01    prod-a    NaN         0\n2    C  2023-05-01    prod-b  256.0        31\n3    D  2023-06-01    prod-a  118.0         0\n4    D  2023-05-15    prod-a  224.0        17",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>date</th>\n      <th>product</th>\n      <th>price</th>\n      <th>days_ago</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n      <td>2023-05-31</td>\n      <td>prod-a</td>\n      <td>17.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B</td>\n      <td>2023-06-01</td>\n      <td>prod-a</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C</td>\n      <td>2023-05-01</td>\n      <td>prod-b</td>\n      <td>256.0</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D</td>\n      <td>2023-06-01</td>\n      <td>prod-a</td>\n      <td>118.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D</td>\n      <td>2023-05-15</td>\n      <td>prod-a</td>\n      <td>224.0</td>\n      <td>17</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#EXAMPLE:  time processing: delta since some starting date. This makes some downstream calcs a little easier\n",
    "df['days_ago'] = df['date'].apply(lambda x: (DATE_AS_OF - x).days)\n",
    "df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:41:34.284016Z",
     "start_time": "2024-03-23T16:41:34.272919Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EXAMPLE: Days since MIN date.   use to normalize. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Date-range min: 2023-01-01\n",
      "Date-range max: 2023-06-01\n"
     ]
    },
    {
     "data": {
      "text/plain": "   user        date   product  price  days_ago  days_since\n0     A  2023-05-31    prod-a   17.0         1         150\n1     B  2023-06-01    prod-a    NaN         0         151\n2     C  2023-05-01    prod-b  256.0        31         120\n3     D  2023-06-01    prod-a  118.0         0         151\n4     D  2023-05-15    prod-a  224.0        17         134\n5     A  2023-02-04    prod-d  939.0       117          34\n6     B  2023-02-07    prod-b   57.0       114          37\n7     C  2023-02-10    prod-a  323.0       111          40\n8     D  2023-02-13    prod-d  949.0       108          43\n9     A  2023-02-16    prod-c  120.0       105          46\n10    B  2023-02-19    prod-a  555.0       102          49\n11    C  2023-02-22    prod-d  692.0        99          52\n12    D  2023-02-25    prod-b  222.0        96          55\n13    A  2023-02-28    prod-a  437.0        93          58\n14    B  2023-03-03    prod-d  631.0        90          61\n15    C  2023-03-06    prod-a  739.0        87          64\n16    D  2023-03-09    prod-d   47.0        84          67\n17    A  2023-03-12    prod-c  309.0        81          70\n18    B  2023-03-15    prod-a  817.0        78          73\n19    A  2023-03-18    prod-d  687.0        75          76\n20    A  2023-03-21    prod-b   91.0        72          79\n21    E  2023-01-01    prod-b   91.0       151           0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>date</th>\n      <th>product</th>\n      <th>price</th>\n      <th>days_ago</th>\n      <th>days_since</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n      <td>2023-05-31</td>\n      <td>prod-a</td>\n      <td>17.0</td>\n      <td>1</td>\n      <td>150</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B</td>\n      <td>2023-06-01</td>\n      <td>prod-a</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>151</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C</td>\n      <td>2023-05-01</td>\n      <td>prod-b</td>\n      <td>256.0</td>\n      <td>31</td>\n      <td>120</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D</td>\n      <td>2023-06-01</td>\n      <td>prod-a</td>\n      <td>118.0</td>\n      <td>0</td>\n      <td>151</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D</td>\n      <td>2023-05-15</td>\n      <td>prod-a</td>\n      <td>224.0</td>\n      <td>17</td>\n      <td>134</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>A</td>\n      <td>2023-02-04</td>\n      <td>prod-d</td>\n      <td>939.0</td>\n      <td>117</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>B</td>\n      <td>2023-02-07</td>\n      <td>prod-b</td>\n      <td>57.0</td>\n      <td>114</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>C</td>\n      <td>2023-02-10</td>\n      <td>prod-a</td>\n      <td>323.0</td>\n      <td>111</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>D</td>\n      <td>2023-02-13</td>\n      <td>prod-d</td>\n      <td>949.0</td>\n      <td>108</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>A</td>\n      <td>2023-02-16</td>\n      <td>prod-c</td>\n      <td>120.0</td>\n      <td>105</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>B</td>\n      <td>2023-02-19</td>\n      <td>prod-a</td>\n      <td>555.0</td>\n      <td>102</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>C</td>\n      <td>2023-02-22</td>\n      <td>prod-d</td>\n      <td>692.0</td>\n      <td>99</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>D</td>\n      <td>2023-02-25</td>\n      <td>prod-b</td>\n      <td>222.0</td>\n      <td>96</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>A</td>\n      <td>2023-02-28</td>\n      <td>prod-a</td>\n      <td>437.0</td>\n      <td>93</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>B</td>\n      <td>2023-03-03</td>\n      <td>prod-d</td>\n      <td>631.0</td>\n      <td>90</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>C</td>\n      <td>2023-03-06</td>\n      <td>prod-a</td>\n      <td>739.0</td>\n      <td>87</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>D</td>\n      <td>2023-03-09</td>\n      <td>prod-d</td>\n      <td>47.0</td>\n      <td>84</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>A</td>\n      <td>2023-03-12</td>\n      <td>prod-c</td>\n      <td>309.0</td>\n      <td>81</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>B</td>\n      <td>2023-03-15</td>\n      <td>prod-a</td>\n      <td>817.0</td>\n      <td>78</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>A</td>\n      <td>2023-03-18</td>\n      <td>prod-d</td>\n      <td>687.0</td>\n      <td>75</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>A</td>\n      <td>2023-03-21</td>\n      <td>prod-b</td>\n      <td>91.0</td>\n      <td>72</td>\n      <td>79</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>E</td>\n      <td>2023-01-01</td>\n      <td>prod-b</td>\n      <td>91.0</td>\n      <td>151</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "LOGGER.info(\"Date-range min: %s\", df['date'].min())\n",
    "LOGGER.info(\"Date-range max: %s\", df['date'].max())\n",
    "\n",
    "dmin = df['date'].min()\n",
    "df['days_since'] = df['date'].apply(lambda x: (x-dmin).days)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:52:02.538699Z",
     "start_time": "2024-03-23T16:52:02.510559Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'datetime.date' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Add number of days since date.   Makes some downstream calcs a little easier\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_all[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdays_ago\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf_all\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATE_AS_OF\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Count unique values in each column\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproduct\u001B[39m\u001B[38;5;124m'\u001B[39m]: \n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/juypter-skeleton-poetry-ZmYnKBkK-py3.9/lib/python3.9/site-packages/pandas/core/series.py:4908\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4780\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4781\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4782\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4787\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4788\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4790\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4791\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4906\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4907\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4908\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4909\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4910\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4911\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4912\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4913\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4914\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4915\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/juypter-skeleton-poetry-ZmYnKBkK-py3.9/lib/python3.9/site-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/juypter-skeleton-poetry-ZmYnKBkK-py3.9/lib/python3.9/site-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/juypter-skeleton-poetry-ZmYnKBkK-py3.9/lib/python3.9/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/juypter-skeleton-poetry-ZmYnKBkK-py3.9/lib/python3.9/site-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn[13], line 2\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Add number of days since date.   Makes some downstream calcs a little easier\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_all[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdays_ago\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df_all[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: (\u001B[43mDATE_AS_OF\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m)\u001B[38;5;241m.\u001B[39mdays)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Count unique values in each column\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproduct\u001B[39m\u001B[38;5;124m'\u001B[39m]: \n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for -: 'datetime.date' and 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Count unique values in each column\n",
    "for col in ['user', 'product']: \n",
    "    LOGGER.info(\"  # unique values in col: %s = %d\",col, df_all[col].nunique())\n",
    "\n",
    "LOGGER.info(\"Date-range min: %s\", df_all['date'].min())\n",
    "LOGGER.info(\"Date-range max: %s\", df_all['date'].max())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T16:19:48.415278Z",
     "start_time": "2024-03-23T16:19:48.033788Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_all.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.296293Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Begin to count the recommend actions. \n",
    "#sort so the largest date value is first in dict.  i.e., we want to recommend the action with the largest date range\n",
    "actions = {k: v for k, v in sorted(ACTION_DATES.items(), key=lambda item: item[1], reverse=True)} \n",
    "default='ok'\n",
    "\n",
    "\n",
    "def determine_action(days:int) ->str: \n",
    "    for action, d_thresehold in actions.items():\n",
    "        if days >= d_thresehold:\n",
    "            return action\n",
    "    return default\n",
    "\n",
    "df_all['action'] = df_all['days_ago'].apply(determine_action)\n",
    "df_all.head(5)\n",
    "\n",
    "#Find the ones beyond the dates. "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.298095Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# sort the actions by oldest\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.300007Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#####################################\n",
    "#most recent purchases per user - any product\n",
    "df_recent = df_all.sort_values(by='date', ascending=False)\n",
    "\n",
    "# group the data by Name and get the first row of each group\n",
    "df_most_recent_purchases = df_recent.groupby('user').first()\n",
    "\n",
    "df_most_recent_purchases\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.301495Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T14:32:10.303797Z",
     "start_time": "2023-12-23T14:32:10.302992Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#####################################\n",
    "#most recent purchases per user - any product\n",
    "\n",
    "df_recent = df_all.sort_values(by='date', ascending=False)\n",
    "\n",
    "# group the data by Name and get the most recent purchase (first row) of each group\n",
    "df_most_recent_purchase = df_recent.groupby(['User'],as_index=False).first()\n",
    "df_most_recent_purchase.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T14:32:10.305649Z",
     "start_time": "2023-12-23T14:32:10.304384Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#####################################\n",
    "#most recent purchases per user - list all products\n",
    "\n",
    "df_recent = df_all.sort_values(by='date', ascending=False)\n",
    "\n",
    "# group the data by Name and get the most recent purchase (first row) of each group\n",
    "df_most_recent_purchases_all = df_recent.groupby(['user', 'product'],as_index=False).first()\n",
    "df_most_recent_purchases_all.head(15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.305865Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Find purchases that are old - such as out of warranty date.\n",
    "filtered_df = df_most_recent_purchases_all.loc[df_most_recent_purchases_all['date'] < '2023-02-01']\n",
    "\n",
    "\n",
    "print(f\"Num purchases out of warranty: {len(filtered_df)}\")\n",
    "filtered_df.head(5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.307693Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Last purchase per user per product\n",
    "#\n",
    "# Note: this doesn't handle case where a user never purchased\n",
    "#####################################\n",
    "#Generate blocks of cutoff days.    The first is the max.\n",
    "dates = []\n",
    "dateblocks = {}\n",
    "\n",
    "dmin = df_all['date'].min().date()\n",
    "print(f\"dmin: Earliest entry from date: {dmin}\")\n",
    "d = DATE_AS_OF\n",
    "ndays = 0\n",
    "while d >= dmin:\n",
    " dates.insert(0,d)\n",
    "\n",
    " prevdate = d - timedelta(days=DATE_GROUP_DAYS)\n",
    " ndays_max=ndays + DATE_GROUP_DAYS-1\n",
    " dateblocks[ndays] = dict(ndays=ndays,\n",
    "                          date=d,\n",
    "                          prevdate=prevdate,\n",
    "                          ndays_max = ndays_max,\n",
    "                          label=f\"{ndays} to {ndays_max} days\")\n",
    "\n",
    " #Next block\n",
    " d = prevdate\n",
    " ndays += DATE_GROUP_DAYS\n",
    "\n",
    " #print(f\"date slice: {d}\")\n",
    " #print(f\"dateblocks: {dateblocks}\")\n",
    "\n",
    " # End up with a dict\n",
    "\n",
    " #values\n",
    " #* n days\n",
    " #* label\n",
    " #df (slice) of entries that match.\n",
    " #* be able to visualize.\n",
    "\n",
    "\n",
    " #<= 10 days:   15\n",
    " #11 <= 20 days  37\n",
    " #21"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.308675Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dateblocks"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.309624Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_all['days_since_login'] = df_all['date'].apply(lambda d: (DATE_AS_OF - d.date()).days)\n",
    "\n",
    "def calc_date_block(ndays):\n",
    "\n",
    "    for d in sorted(dateblocks.keys()):\n",
    "        #print(f\"calc_date_block d={str(d)}  v={dateblocks.get(d)}\")\n",
    "        v = dateblocks[d]\n",
    "        if ndays <= v.get('ndays_max'):\n",
    "            return v.get('label')\n",
    "    return ('unknown')\n",
    "\n",
    "df_all['days_since_label'] = df_all['days_since_login'].apply(calc_date_block)\n",
    "\n",
    "df_all.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.310680Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_date_bands = df_all.groupby('days_since_label',as_index=False).count()\n",
    "df_date_bands.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.311738Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sns.barplot(x='days_since_label', y='User', data=df_date_bands) #hue='variable')\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.ylabel('Returns')\n",
    "#plt.title('Portfolio vs Benchmark Returns');"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.312919Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.313926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.314922Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = filtered_df\n",
    "df\n",
    "\n",
    " Filter data between two dates\n",
    "filtered_df = df.loc[(df['date'] >= '2020-09-01')\n",
    "                     & (df['date'] < '2020-09-15')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.316104Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_all['date'].min()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.317089Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_dict = {'a': 3, 'b': 1, 'c': 2}\n",
    "sorted_dict = {k: v for k, v in sorted(my_dict.items(), key=lambda item: item[1], reverse=True)}  # Sort by values\n",
    "print(sorted_dict)  # Output: {'b': 1, 'c': 2, 'a': 3}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.318177Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-23T14:32:10.319311Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0 (main, Oct 25 2022, 13:57:33) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
